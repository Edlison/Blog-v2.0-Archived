<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/man-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/man-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.1/css/all.min.css">

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"blog.edlison.com","root":"/","images":"/images","scheme":"Muse","version":"8.0.2","exturl":true,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}};
  </script>

  <meta name="description" content="Edlison blog">
<meta property="og:type" content="website">
<meta property="og:title" content="Edlison is talking here.">
<meta property="og:url" content="https://blog.edlison.com/page/43/index.html">
<meta property="og:site_name" content="Edlison is talking here.">
<meta property="og:description" content="Edlison blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Bolin Shen">
<meta property="article:tag" content="Edlison">
<meta property="article:tag" content="edlison">
<meta property="article:tag" content="blog">
<meta property="article:tag" content="code">
<meta property="article:tag" content="solution">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://blog.edlison.com/page/43/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Edlison is talking here. - “Edlison's blog”</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-NC6FWQJNCW"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-NC6FWQJNCW');
      }
    </script>






  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Edlison is talking here.</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">“Edlison's blog”</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <section class="post-toc-wrap sidebar-panel">
        </section>
        <!--/noindex-->

        <section class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Bolin Shen</p>
  <div class="site-description" itemprop="description">Edlison blog</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">286</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">25</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">142</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2VkbGlzb24=" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;edlison"><i class="fab fa-github fa-fw"></i>GitHub</span>
      </span>
      <span class="links-of-author-item">
        <span class="exturl" data-url="bWFpbHRvOmJvbGluLmVkbGlzb24uc2hlbkBnbWFpbC5jb20=" title="E-Mail → mailto:bolin.edlison.shen@gmail.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</span>
      </span>
  </div>



        </section>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner index posts-expand">
      

      
    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blog.edlison.com/dev/ml/deep_learning/BERT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bolin Shen">
      <meta itemprop="description" content="Edlison blog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Edlison is talking here.">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/dev/ml/deep_learning/BERT/" class="post-title-link" itemprop="url">dev/ml/deep_learning/BERT</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 09-23-2020 11:57:29 / Modified: 22:18:02" itemprop="dateCreated datePublished" datetime="2020-09-23T11:57:29+08:00">09-23-2020</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="BERT-概述"><a href="#BERT-概述" class="headerlink" title="BERT 概述"></a>BERT 概述</h1><p>BERT的全称是Bidirectional Encoder Representation from Transformers，即双向Transformer的Encoder，因为decoder是不能获要预测的信息的。模型的主要创新点都在pre-train方法上，即用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representation。</p>
<p>训练方法：</p>
<ul>
<li>它在训练双向语言模型时以减小的概率把少量的词替成了Mask或另一个随机的单词。</li>
<li>增加了一个预测下一句的loss。</li>
</ul>
<p>特点：</p>
<ul>
<li>模型深，12层，中间层只有1024wide。Transformer的中间层有2048。（深而窄 比 浅而宽 的模型更好？）</li>
<li>MLM(Masked Language Model)，同时利用左侧和右侧的词语。</li>
</ul>
<hr>
<p>Reference</p>
<p>BERT | Bidirectional Encoder Representation from Transformers<br><span class="exturl" data-url="aHR0cHM6Ly9lYXN5YWkudGVjaC9haS1kZWZpbml0aW9uL2JlcnQv">https://easyai.tech/ai-definition/bert/<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blog.edlison.com/dev/ml/deep_learning/Transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bolin Shen">
      <meta itemprop="description" content="Edlison blog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Edlison is talking here.">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/dev/ml/deep_learning/Transformer/" class="post-title-link" itemprop="url">dev/ml/deep_learning/Transformer</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 09-23-2020 11:57:22" itemprop="dateCreated datePublished" datetime="2020-09-23T11:57:22+08:00">09-23-2020</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 10-10-2021 22:35:09" itemprop="dateModified" datetime="2021-10-10T22:35:09+08:00">10-10-2021</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Transfomer-概述"><a href="#Transfomer-概述" class="headerlink" title="Transfomer 概述"></a>Transfomer 概述</h1><p>和经典的 seq2seq 模型一样，Transformer 模型中也采用了 encoer-decoder 架构。上图的左半边用 NX 框出来的，就代表一层 encoder，其中论文里面的 encoder 一共有6层这样的结构。上图的右半边用 NX 框出来的，则代表一层 decoder，同样也有6层。</p>
<p>定义输入序列首先经过 word embedding，再和 positional encoding 相加后，输入到 encoder 中。输出序列经过的处理和输入序列一样，然后输入到 decoder。</p>
<p>最后，decoder 的输出经过一个线性层，再接 Softmax。</p>
<p>于上便是 Transformer 的整体框架，下面先来介绍 encoder 和 decoder。</p>
<p><strong>Encoder</strong></p>
<p>由 6 层相同的层组成，每一层分别由两部分组成：</p>
<ul>
<li><p>第一部分是 multi-head self-attention</p>
</li>
<li><p>第二部分是 position-wise feed-forward network，是一个全连接层</p>
</li>
</ul>
<p>两个部分，都有一个残差连接(residual connection)，然后接着一个 Layer Normalization。</p>
<p><strong>Decoder</strong></p>
<p>和 encoder 类似，decoder 也是由6个相同的层组成，每一层由三个部分组成：</p>
<ul>
<li>第一个部分是 multi-head self-attention mechanism</li>
<li>第二部分是 multi-head context-attention mechanism</li>
<li>第三部分是一个 position-wise feed-forward network和encoder 一样，上面三个部分的每一个部分，都有一个残差连接，后接一个 Layer Normalization。</li>
</ul>
<p>decoder 和 encoder 不同的地方在 multi-head context-attention mechanism</p>
<p><strong>Attention</strong></p>
<p>Attention用一句话来描述，就是encoder层的输出经过加权平均后再输入到decoder层中。它主要应用在seq2seq模型中，这个加权可以用矩阵来表示，也叫Attention矩阵。它表示对于某个时刻的输出y，他在输入x上各个部分的注意力。这个注意力就是加权。</p>
<p><strong>Self-Attention</strong></p>
<p>attention有两个隐状态，hi和st。前者是输入序列第i个位置产生的隐状态，后者是输出序列在第t个位置产生的隐状态。self-attention就是，输出序列就是输入序列。自己计算自己的attention得分。</p>
<p><strong>Context-Attention</strong></p>
<p>context-attention是encoder和decoder之间的attention，是两个不同序列之间的attention，与来源于自身的self-attention相区别。</p>
<p>计算attention权重的方法：</p>
<ul>
<li>additive attention</li>
<li>local-base</li>
<li>general</li>
<li>dot-product</li>
<li>scaled dot-product</li>
</ul>
<p><strong>Scaled Dot-Product Attention</strong></p>
<p>通过query和key的相似性程度来确定value的权重分布。</p>
<p><img src="https://pic4.zhimg.com/80/v2-e2bc3a470b359e8bdce750843140897e_720w.jpg" alt="img-scaled-dot-attention"></p>
<hr>
<p>Reference</p>
<p>Transformer<br><span class="exturl" data-url="aHR0cHM6Ly9lYXN5YWkudGVjaC9haS1kZWZpbml0aW9uL3RyYW5zZm9ybWVyLw==">https://easyai.tech/ai-definition/transformer/<i class="fa fa-external-link-alt"></i></span></p>
<p>Attention<br><span class="exturl" data-url="aHR0cHM6Ly9lYXN5YWkudGVjaC9haS1kZWZpbml0aW9uL2F0dGVudGlvbi8=">https://easyai.tech/ai-definition/attention/<i class="fa fa-external-link-alt"></i></span></p>
<p>EncoderDecoder Seq2Seq</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9lYXN5YWkudGVjaC9haS1kZWZpbml0aW9uL2VuY29kZXItZGVjb2Rlci1zZXEyc2VxLw==">https://easyai.tech/ai-definition/encoder-decoder-seq2seq/<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blog.edlison.com/dev/ml/deep_learning/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bolin Shen">
      <meta itemprop="description" content="Edlison blog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Edlison is talking here.">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/dev/ml/deep_learning/%E6%96%87%E6%9C%AC%E8%A1%A8%E7%A4%BA/" class="post-title-link" itemprop="url">dev/ml/deep_learning/文本表示</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 09-23-2020 11:47:08" itemprop="dateCreated datePublished" datetime="2020-09-23T11:47:08+08:00">09-23-2020</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 11-12-2020 00:30:47" itemprop="dateModified" datetime="2020-11-12T00:30:47+08:00">11-12-2020</time>
      </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="NLP中处理文本方法"><a href="#NLP中处理文本方法" class="headerlink" title="NLP中处理文本方法"></a>NLP中处理文本方法</h1><p>由于文本是一种非架构化的数据，我们要将这种非结构化的数据转化为结构化的信息，这样我们才能对其进行处理。</p>
<p>现在文本表示大致有三种方式。</p>
<p><img src="https://easy-ai.oss-cn-shanghai.aliyuncs.com/2020-02-17-guanxi.png" alt="img"></p>
<h2 id="One-Hot"><a href="#One-Hot" class="headerlink" title="One-Hot"></a>One-Hot</h2><p><strong>向量中每一个位置代表一个词</strong></p>
<p><strong>one-hot的缺点：</strong></p>
<ul>
<li>无法表达词语之间的关系</li>
<li>这种过于稀疏的向量，导致计算和存储的效率不高</li>
</ul>
<h2 id="整数编码"><a href="#整数编码" class="headerlink" title="整数编码"></a>整数编码</h2><p><strong>整数编码的缺点</strong></p>
<ul>
<li>无法表达词语之间的关系</li>
<li>对于模型解释而言，整数编码可能具有挑战性</li>
</ul>
<h2 id="Word-Embedding-词嵌入"><a href="#Word-Embedding-词嵌入" class="headerlink" title="Word Embedding (词嵌入)"></a>Word Embedding (词嵌入)</h2><p>优点：</p>
<ul>
<li>可以将文本通过一个低维向量来表达，不像one-hot长</li>
<li>语义相似的词在向量空间上比较相近</li>
<li>通用性强，可用在不同的任务中</li>
</ul>
<h3 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h3><p>基于统计方法来获得词向量的方法。</p>
<p>两种训练模式：</p>
<ul>
<li>通过上下文来预测当前词(CBOW)</li>
<li>通过当前词来预测上下文(Skip-gram)</li>
</ul>
<p>提高速度的优化方法：</p>
<ul>
<li>Negtive Sample (负采样)</li>
<li>Hierarchical Softmax</li>
</ul>
<p>优点：</p>
<ul>
<li>由于Word2Vec会考虑上下文，跟之前的Embedding方法相比，效果要更好</li>
<li>比之前的Embedding方法维度更少，速度更快</li>
<li>通用性强，用于各种NLP任务</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>由于词和向量是一对一的关系，所以多义词的问题无法解决</li>
<li>Word2Vec是一种静态的方式，虽然通用性强，但是无法针对特定任务做动态优化</li>
</ul>
<h3 id="GloVe"><a href="#GloVe" class="headerlink" title="GloVe"></a>GloVe</h3><p>实现有三步：</p>
<ul>
<li>根据语料库构建一个共现矩阵，矩阵中的每一个元素代表单词和上下文单词在特定大小的上下文窗口内共同出现的次数。</li>
<li>构建词向量和共现矩阵之间的近似关系。</li>
<li>loss function</li>
</ul>
<p>Reference</p>
<p>词嵌入 | Word embedding</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9lYXN5YWkudGVjaC9haS1kZWZpbml0aW9uL3dvcmQtZW1iZWRkaW5nLw==">https://easyai.tech/ai-definition/word-embedding/<i class="fa fa-external-link-alt"></i></span></p>
<p>Word2vec</p>
<p><span class="exturl" data-url="aHR0cHM6Ly9lYXN5YWkudGVjaC9haS1kZWZpbml0aW9uL3dvcmQydmVjLw==">https://easyai.tech/ai-definition/word2vec/<i class="fa fa-external-link-alt"></i></span></p>
<p>GloVe详解</p>
<p><span class="exturl" data-url="aHR0cDovL3d3dy5mYW55ZW9uZy5jb20vMjAxOC8wMi8xOS9nbG92ZS1pbi1kZXRhaWwv">http://www.fanyeong.com/2018/02/19/glove-in-detail/<i class="fa fa-external-link-alt"></i></span></p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blog.edlison.com/academia/paper_learning/Attention-Transformer/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bolin Shen">
      <meta itemprop="description" content="Edlison blog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Edlison is talking here.">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/academia/paper_learning/Attention-Transformer/" class="post-title-link" itemprop="url">academia/paper_learning/Attention-Transformer</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 09-18-2020 21:39:50 / Modified: 21:40:18" itemprop="dateCreated datePublished" datetime="2020-09-18T21:39:50+08:00">09-18-2020</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1>
      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://blog.edlison.com/academia/paper_learning/XLNet/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bolin Shen">
      <meta itemprop="description" content="Edlison blog">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Edlison is talking here.">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/academia/paper_learning/XLNet/" class="post-title-link" itemprop="url">XLNet</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 09-18-2020 17:33:11" itemprop="dateCreated datePublished" datetime="2020-09-18T17:33:11+08:00">09-18-2020</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 02-04-2021 13:49:16" itemprop="dateModified" datetime="2021-02-04T13:49:16+08:00">02-04-2021</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/" itemprop="url" rel="index"><span itemprop="name">Paper</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Paper/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a>
        </span>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <h1 id="XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding"><a href="#XLNet-Generalized-Autoregressive-Pretraining-for-Language-Understanding" class="headerlink" title="XLNet Generalized Autoregressive Pretraining for Language Understanding"></a>XLNet Generalized Autoregressive Pretraining for Language Understanding</h1><h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>Bert基于去噪自编码器的预训练模型，性能优于自回归语言模型的预训练方法。而由于mask一部分输入，bert忽略了被mask位置之间的依赖关系。</p>
<p>XLNet基于此优缺点可以</p>
<ul>
<li>通过最大化所有可能的因式分解顺序的对数似然，学习双向语境信息</li>
<li>用自回归本身的特点克服bert的缺点</li>
<li>融合了自回归模型Transformer-XT的思路</li>
</ul>
<p>作者从自回归(autoregressive)和自编码(autoencoding)两大范式分析了当前的预训练语言模型。</p>
<h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h1><h2 id="1-1-AutoRegression与AutoEncoding两大阵营"><a href="#1-1-AutoRegression与AutoEncoding两大阵营" class="headerlink" title="1.1 AutoRegression与AutoEncoding两大阵营"></a>1.1 AutoRegression与AutoEncoding两大阵营</h2><p>XLNet是一个类似BERT的模型，XLNet是一种通用的自回归预训练方法。</p>
<h3 id="自回归-AutoRegression-语言模型"><a href="#自回归-AutoRegression-语言模型" class="headerlink" title="自回归(AutoRegression)语言模型"></a>自回归(AutoRegression)语言模型</h3><p>AR语言模型是一种使用上下文词来预测下一个词的模型，但是上下文单词被限制在两个方向，前向和后向。(例如：GPT，GPT-2)</p>
<p><img src="https://img-blog.csdnimg.cn/20190810224951812.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNzk0NzE1Ng==,size_16,color_FFFFFF,t_70" alt="img-前向后向"></p>
<p>AR语言模型擅长生成式NLP任务，生成上下文时总是前向的。</p>
<p>但是AR只能使用前向上下文或后向上下文，这意味着它不能同时使用前向和后向上下文。</p>
<h3 id="自编码-AutoEncoding-语言模型"><a href="#自编码-AutoEncoding-语言模型" class="headerlink" title="自编码(AutoEncoding)语言模型"></a>自编码(AutoEncoding)语言模型</h3><p>BERT被归类为自编码器语言模型。AE旨在从损坏的输入中重建原始数据。</p>
<p>损坏的输入意味着我们在预训练阶段用[MASK]替换原始词，目标是预测得到原始句子。<br><img src="https://img-blog.csdnimg.cn/20190810225020688.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNzk0NzE1Ng==,size_16,color_FFFFFF,t_70" alt="img-双向"></p>
<p>AE语言模型的优势是，它可以从前向和后向的方向看到上下文。</p>
<p>AE语言模型的缺点是</p>
<ul>
<li>[MASK]这种人为的符号在调优时在真实数据中并不存在，会导致预训练-调优的差异(pretrain-finetune discrepancy)</li>
<li>[MASK]假设mask词在给定未mask词的情况下彼此独立（例：住房危机已经变成银行业危机。 其忽略了 银行业 与 危机 间的关系）。</li>
</ul>
<h2 id="1-2-两大阵营需要新的XLNet"><a href="#1-2-两大阵营需要新的XLNet" class="headerlink" title="1.2 两大阵营需要新的XLNet"></a>1.2 两大阵营需要新的XLNet</h2><p>XLNet提出了一种让AR语言模型从双向上下文中学习的新方法，以避免[MASK]方法在AE语言模型中带来的缺点。</p>
<p>XLNet是一种泛化自回归(AR)方法，既集合了AR和AE方法的优势，又避免了二者的缺陷。</p>
<ul>
<li>XLNet不使用传统AR模型中固定的前向或后向因式分解顺序，而是最大化所有可能因式分解顺序的期望对数似然，由于对因式分解顺序的排列操作，每个位置的语境都包含来自左侧和右侧的token。因此，每个位置都能学习来自所有位置的语境信息，即捕获双向语境。</li>
<li>作为一个泛化自回归(AR)模型，XLNet不依赖残缺数据。不会有BERT的预训练-微调差异。自回归目标提供一种自然的方式，来利用乘法法则对预测token的联合概率执行因式分解(factorize)，这消除了BERT中的独立性假设。</li>
<li>XLNet将Transformer-XL的分割循环机制和相对编码范式整合到预训练中。</li>
</ul>
<h1 id="2-Proposed-Solution"><a href="#2-Proposed-Solution" class="headerlink" title="2 Proposed Solution"></a>2 Proposed Solution</h1><h2 id="2-1-在自回归模型中引入双向语言模型基本思想"><a href="#2-1-在自回归模型中引入双向语言模型基本思想" class="headerlink" title="2.1 在自回归模型中引入双向语言模型基本思想"></a>2.1 在自回归模型中引入双向语言模型基本思想</h2><p>XLNet仍然遵循两阶段的过程(pretrain fine-tune)。其主要希望改动第一阶段的过程，即不像BERT那样带[MASK]的Denoising-autoencoder的模式，而是采用AR语言模型的模式。</p>
<p>简单来说就是，输入句子X仍然使自左向右的输入，看到Ti单词的上文Context-before来预测Ti这个单词，又希望Context-before例不仅看到上文单词又能看到后面的Context-after里的下文单词。这样一来BERT预训练阶段引入的[MASK]符号就不需要了。</p>
<p>基本思想：在预训练阶段引入Permutation LM的训练目标。</p>
<p>举例：比如包含单词Ti的当前输入的句子X，由顺序的几个单词构成x1, x2, x3, x4四个单词顺序组成。假设x3为要预测的单词Ti，其所在的位置为Position 3，要想让它能够在上文Context-before中，也就是Pos 1或Pos 2的位置看到Pos 4的单词x4。可以这么做，假设固定住x3的位置，也就是仍在x3，之后随机排列组合句子中的4个单词，在所有可能里训责一部分作为模型的预训练的输入X。比如随机排列组合后x4, x2, x3, x1这样一个排列组合作为模型的输入X。于是x3就能同时看到上文x2以及下文x4的内容了。</p>
<p><img src="https://pic3.zhimg.com/80/v2-05d785e9d8f810d118e4fa93f8e9b39f_1440w.jpg" alt="img-permutation-language-model"></p>
<p>看上去仍然是个自回归的从左到右的模型，但是其实通过对句子中单词排列组合，把一部分Ti下文的单词排到Ti上文位置中，于是就看到了上文和下文，但是形式上看上去仍然是从左到右在预测后一个单词。</p>
<h2 id="2-2-具体实现"><a href="#2-2-具体实现" class="headerlink" title="2.2 具体实现"></a>2.2 具体实现</h2><p>尽管基本思想里提到把句子X的单词排列组合后，再随机抽取例子作为输入，但实际上不可以，因为Fine-tune阶段不可能去排列组合原始输入。所以必须让预训练阶段的输入部分<strong>看上去</strong>仍然是x1, x2, x3, x4这个顺序，但是可以在Transformer部分做些工作，来达成目标。</p>
<p>XLNet采取了Attention掩码机制，输入的X没有任何变化，单词Ti还是第i个单词，前面有1到i-1个单词。但是Transformer内部，通过Attention掩码从X的输入单词里，也就是Ti的Context-before和Context-after中随机选择i-1个单词，放到Ti的Context-before上，把其他单词的输入通过Attention掩码隐藏掉。（当然这个所谓放到Ti的上文位置，只是一种形象的说法，其实在内部，就是通过Attention Mask，把其它没有被选到的单词Mask掉，不让它们在预测单词Ti的时候发生作用，如此而已。看着就类似于把这些被选中的单词放到了上文Context_before的位置了）。</p>
<p>XLNet是用<strong>双流自注意力模型</strong>实现的。一个是内容流自注意力，其实就是标准的Transformer的计算过程，主要引入了Query流自注意力。其实就是用来替代[MASK]标记的，XLNet因为要抛弃[MASK]又不能看到x3的输入，于是Query流就直接忽略掉x3的输入，只保留这个位置信息，用参数w来代表位置的embedding编码。<strong>其实XLNet只是抛弃了[MASK]这个占位符号，内部还是引入Query流来忽略掉被MASK的这个单词。和BERT比只是实现方式不同而已</strong>。</p>
<p><img src="https://picb.zhimg.com/80/v2-2bb1a60af4fe2fa751647fdce48e337c_1440w.jpg" alt="img-two-stream-attention"></p>
<p>如图所示，输入序列仍是x1, x2, x3, x4，通过不同的掩码矩阵，让当前单词Xi只能看到被排列组合后的顺序x3-&gt;x2-&gt;x4-&gt;x1中自己前面的单词。</p>
<h2 id="2-3-XLNet与BERT"><a href="#2-3-XLNet与BERT" class="headerlink" title="2.3 XLNet与BERT"></a>2.3 XLNet与BERT</h2><p>通过改造BERT预训练的过程，其实可以模拟XLNet的PLM过程。</p>
<p>第一种思路：Bert目前的做法是，给定输入句子X，随机Mask掉15%的单词，然后要求利用剩下的85%的单词去预测任意一个被Mask掉的单词。对于输入句子，随机选择X中的任意i个单词，只用这i个单词去预测被Mask的单词。这个过程理论上可以在Transformer内采用Attention Mask来实现，这样BERT的预训练模型就和XLNet基本等价了。</p>
<p>第二种思路：假设仍使用BERT目前的Mask机制，将Mask 15%改为，每次一个句子只Mask掉一个单词，利用剩下的单词来预测被Mask掉的单词。因为XLNet在实现的时候，为了提升效率，其实是选择每个句子最后末尾的1/K个单词被预测，假设K=7，意味着一个句子X，只有末尾的1/7的单词会被预测。这样两者基本是等价的。</p>
<p>XLNet维持了自回归(AR)语言模型的从左向右的模式，这个BERT做不到。明显的好处是，对于生成类的任务，能够在维持表面从左向右的生成过程前提下，模型里隐含了上下文的信息。XLNet貌似应该对于生成类型的NLP任务，会比BERT又明显优势。此外，XLNet还引入了Transformer XL机制，所以对于长文档输入类型的NLP任务，也会比BERT有明显优势。</p>
<h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3 总结"></a>3 总结</h2><p>Permutation Language Model是XLNet的主要理论创新，<strong>它开启了自回归(AR)语言模型如何引入下文的一个思路</strong>。</p>
<p>XLNet也相当于是BERT, GPT2.0, Transformer XL的综合体。</p>
<ul>
<li>通过Permutation LM 预训练目标，吸收了BERT的双向语言模型；</li>
<li>吸收了GPT2.0使用更多更高质量的预训练数据；</li>
<li>吸收了Transformer XL的主要思想。</li>
</ul>
<p>XLNet其实本质上还是ELMO/GPT/BERT这一系列两阶段模型的进一步延伸。在自回归(AR)语言模型方向引入双向语言模型打开了新思路。</p>
<p>未来预期</p>
<ul>
<li>Transformer天然对长文档任务处理有弱点，所以XLNet对于长文档NLP任务相比BERT应该有直接且比较明显的性能提升；</li>
<li>对于生成类的NLP任务，XLNet的预训练模式符合下游任务序列生成结果。</li>
</ul>
<hr>
<p>Rreference</p>
<p><span class="exturl" data-url="aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC83MDI1NzQyNw==">https://zhuanlan.zhihu.com/p/70257427<i class="fa fa-external-link-alt"></i></span></p>
<p><span class="exturl" data-url="aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNzk0NzE1Ni9hcnRpY2xlL2RldGFpbHMvOTMwMzU2MDc=">https://blog.csdn.net/weixin_37947156/article/details/93035607<i class="fa fa-external-link-alt"></i></span></p>
<p>// AR AE LM 实现步骤？<br>// TODO Transformer 较于 RNN？<br>// TODO encoding decoding？<br>// TODO denoising-autoencoder?  </p>
<p>// TODO Transformer-XL 分割循环机制 相对编码范式??<br>// TODO BERT 预训练-微调两阶段模式(pretrain fine-tune) 抛弃mask使两阶段保持一致？ fine-tune阶段？<br>// TODO AR模型前向后向因式分解顺序？  </p>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>





  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/42/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/42/">42</a><span class="page-number current">43</span><a class="page-number" href="/page/44/">44</a><span class="space">&hellip;</span><a class="page-number" href="/page/58/">58</a><a class="extend next" rel="next" href="/page/44/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



      

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

    </div>
  </main>

  <footer class="footer">
    <div class="footer-inner">
      

      

<div class="copyright">
  
  &copy; 2018 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Edlison</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  
  <script>
    (function(){
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      bp.src = (curProtocol === 'https') ? 'https://zz.bdstatic.com/linksubmit/push.js' : 'http://push.zhanzhang.baidu.com/push.js';
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>


















  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>








  

  

</body>
</html>
